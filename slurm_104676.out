Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: lightning-bolts in /home/ga947/.local/lib/python3.9/site-packages (0.6.0.post1)
Requirement already satisfied: lightning-utilities!=0.4.0,>=0.3.0 in /home/ga947/.local/lib/python3.9/site-packages (from lightning-bolts) (0.4.2)
Requirement already satisfied: torchvision>=0.10.0 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from lightning-bolts) (0.13.0a0+8069656)
Requirement already satisfied: pytorch-lightning>=1.7.0 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from lightning-bolts) (1.7.7)
Requirement already satisfied: torch>=1.9.* in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from pytorch-lightning>=1.7.0->lightning-bolts) (1.12.1.post201)
Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from pytorch-lightning>=1.7.0->lightning-bolts) (2022.11.0)
Requirement already satisfied: tqdm>=4.57.0 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from pytorch-lightning>=1.7.0->lightning-bolts) (4.64.1)
Requirement already satisfied: typing-extensions>=4.0.0 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from pytorch-lightning>=1.7.0->lightning-bolts) (4.4.0)
Requirement already satisfied: torchmetrics>=0.7.0 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from pytorch-lightning>=1.7.0->lightning-bolts) (0.10.2)
Requirement already satisfied: PyYAML>=5.4 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from pytorch-lightning>=1.7.0->lightning-bolts) (6.0)
Requirement already satisfied: packaging>=17.0 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from pytorch-lightning>=1.7.0->lightning-bolts) (21.3)
Requirement already satisfied: tensorboard>=2.9.1 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from pytorch-lightning>=1.7.0->lightning-bolts) (2.11.0)
Requirement already satisfied: numpy>=1.17.2 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from pytorch-lightning>=1.7.0->lightning-bolts) (1.23.4)
Requirement already satisfied: pyDeprecate>=0.3.1 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from pytorch-lightning>=1.7.0->lightning-bolts) (0.3.2)
Requirement already satisfied: requests in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from torchvision>=0.10.0->lightning-bolts) (2.28.1)
Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from torchvision>=0.10.0->lightning-bolts) (9.2.0)
Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.7.0->lightning-bolts) (3.8.3)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from packaging>=17.0->pytorch-lightning>=1.7.0->lightning-bolts) (3.0.9)
Requirement already satisfied: wheel>=0.26 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (0.38.4)
Requirement already satisfied: google-auth<3,>=1.6.3 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (2.14.1)
Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (0.6.0)
Requirement already satisfied: protobuf in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (4.21.9)
Requirement already satisfied: markdown>=2.6.8 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (3.4.1)
Requirement already satisfied: grpcio>=1.24.3 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (1.49.1)
Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (1.8.1)
Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (0.4.6)
Requirement already satisfied: setuptools>=41.0.0 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (65.5.1)
Requirement already satisfied: absl-py>=0.4 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (1.3.0)
Requirement already satisfied: werkzeug>=1.0.1 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (2.2.2)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from requests->torchvision>=0.10.0->lightning-bolts) (1.26.11)
Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from requests->torchvision>=0.10.0->lightning-bolts) (3.4)
Requirement already satisfied: charset-normalizer<3,>=2 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from requests->torchvision>=0.10.0->lightning-bolts) (2.1.1)
Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from requests->torchvision>=0.10.0->lightning-bolts) (2022.9.24)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.7.0->lightning-bolts) (4.0.2)
Requirement already satisfied: yarl<2.0,>=1.0 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.7.0->lightning-bolts) (1.8.1)
Requirement already satisfied: multidict<7.0,>=4.5 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.7.0->lightning-bolts) (6.0.2)
Requirement already satisfied: attrs>=17.3.0 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.7.0->lightning-bolts) (22.1.0)
Requirement already satisfied: aiosignal>=1.1.2 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.7.0->lightning-bolts) (1.3.1)
Requirement already satisfied: frozenlist>=1.1.1 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.7.0->lightning-bolts) (1.3.3)
Requirement already satisfied: cachetools<6.0,>=2.0.0 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (5.2.0)
Requirement already satisfied: rsa<5,>=3.1.4 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (4.9)
Requirement already satisfied: pyasn1-modules>=0.2.1 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (0.2.7)
Requirement already satisfied: six>=1.9.0 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (1.16.0)
Requirement already satisfied: requests-oauthlib>=0.7.0 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (1.3.1)
Requirement already satisfied: importlib-metadata>=4.4 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (5.0.0)
Requirement already satisfied: MarkupSafe>=2.1.1 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (2.1.1)
Requirement already satisfied: zipp>=0.5 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (3.10.0)
Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (0.4.8)
Requirement already satisfied: oauthlib>=3.0.0 in /ext3/miniconda3/envs/project/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning>=1.7.0->lightning-bolts) (3.2.2)
Found 700 training images.
Dimensions for backbone OK. Note that code for training backbone computes effective batch size per GPU.
{'path_lbl': 'labeled_data/', 'path_unlbl': 'unlabeled_data/', 'shuffle': False, 'verbose_off': True, 'classes': 100, 'train_backbone': True, 'output_size': 1, 'backbone': 'SimCLR', 'batch_size': 64, 'epochs': 5, 'learn_rate': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005, 'print_freq': 200, 'step': 50, 'gamma': 0.2, 'freeze': False, 'backbone_epochs': 1, 'backbone_cuda_off': True, 'backbone_seed': 77777, 'backbone_img_size': 224, 'backbone_save_directory': 'saved_models/', 'backbone_load_pretrained': False, 'backbone_grad_accumulate_steps': 1, 'backbone_batch_size': 4, 'backbone_embedding_size': 128, 'backbone_lr': 0.003, 'backbone_weight_decay': 1e-06, 'backbone_temperature': 0.1, 'backbone_checkpoint_path': './SimCLR_ResNet18.ckpt', 'backbone_resume': False}
Total effective batch size (across all GPU(s)) is 16
Pretraining backbone...
available_gpus: 4
Dim MLP input: 512
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name  | Type            | Params
------------------------------------------
0 | model | AddProjection   | 11.5 M
1 | loss  | ContrastiveLoss | 0     
------------------------------------------
11.5 M    Trainable params
0         Non-trainable params
11.5 M    Total params
46.024    Total estimated model params size (MB)
Optimizer Adam, Learning Rate 0.003, Effective batch size 4
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/63 [00:00<?, ?it/s] Optimizer Adam, Learning Rate 0.003, Effective batch size 4
Optimizer Adam, Learning Rate 0.003, Effective batch size 4
Optimizer Adam, Learning Rate 0.003, Effective batch size 4
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Epoch 0:   2%|▏         | 1/63 [00:24<25:37, 24.79s/it]Epoch 0:   2%|▏         | 1/63 [00:24<25:37, 24.80s/it, loss=4.08, v_num=104676, Contrastive loss_step=4.080]Epoch 0:   3%|▎         | 2/63 [00:24<12:37, 12.42s/it, loss=4.08, v_num=104676, Contrastive loss_step=4.080]Epoch 0:   3%|▎         | 2/63 [00:24<12:37, 12.42s/it, loss=4.67, v_num=104676, Contrastive loss_step=5.250]Epoch 0:   5%|▍         | 3/63 [00:24<08:17,  8.29s/it, loss=4.67, v_num=104676, Contrastive loss_step=5.250]Epoch 0:   5%|▍         | 3/63 [00:24<08:17,  8.29s/it, loss=4.55, v_num=104676, Contrastive loss_step=4.320]Epoch 0:   6%|▋         | 4/63 [00:24<06:07,  6.23s/it, loss=4.55, v_num=104676, Contrastive loss_step=4.320]Epoch 0:   6%|▋         | 4/63 [00:24<06:07,  6.23s/it, loss=4.18, v_num=104676, Contrastive loss_step=3.040]Epoch 0:   8%|▊         | 5/63 [00:24<04:49,  4.99s/it, loss=4.18, v_num=104676, Contrastive loss_step=3.040]Epoch 0:   8%|▊         | 5/63 [00:24<04:49,  4.99s/it, loss=3.75, v_num=104676, Contrastive loss_step=2.070]Epoch 0:  10%|▉         | 6/63 [00:25<03:57,  4.17s/it, loss=3.75, v_num=104676, Contrastive loss_step=2.070]Epoch 0:  10%|▉         | 6/63 [00:25<03:57,  4.17s/it, loss=3.95, v_num=104676, Contrastive loss_step=4.950]Epoch 0:  11%|█         | 7/63 [00:25<03:20,  3.58s/it, loss=3.95, v_num=104676, Contrastive loss_step=4.950]Epoch 0:  11%|█         | 7/63 [00:25<03:20,  3.58s/it, loss=3.87, v_num=104676, Contrastive loss_step=3.360]Epoch 0:  13%|█▎        | 8/63 [00:25<02:52,  3.14s/it, loss=3.87, v_num=104676, Contrastive loss_step=3.360]Epoch 0:  13%|█▎        | 8/63 [00:25<02:52,  3.14s/it, loss=4.13, v_num=104676, Contrastive loss_step=5.940]Epoch 0:  14%|█▍        | 9/63 [00:25<02:30,  2.79s/it, loss=4.13, v_num=104676, Contrastive loss_step=5.940]Epoch 0:  14%|█▍        | 9/63 [00:25<02:30,  2.79s/it, loss=3.9, v_num=104676, Contrastive loss_step=2.100] Epoch 0:  16%|█▌        | 10/63 [00:25<02:13,  2.52s/it, loss=3.9, v_num=104676, Contrastive loss_step=2.100]Epoch 0:  16%|█▌        | 10/63 [00:25<02:13,  2.52s/it, loss=3.87, v_num=104676, Contrastive loss_step=3.550]Epoch 0:  17%|█▋        | 11/63 [00:25<01:59,  2.29s/it, loss=3.87, v_num=104676, Contrastive loss_step=3.550]Epoch 0:  17%|█▋        | 11/63 [00:25<01:59,  2.29s/it, loss=3.97, v_num=104676, Contrastive loss_step=4.980]Epoch 0:  19%|█▉        | 12/63 [00:25<01:47,  2.11s/it, loss=3.97, v_num=104676, Contrastive loss_step=4.980]Epoch 0:  19%|█▉        | 12/63 [00:25<01:47,  2.11s/it, loss=3.74, v_num=104676, Contrastive loss_step=1.180]Epoch 0:  21%|██        | 13/63 [00:25<01:37,  1.95s/it, loss=3.74, v_num=104676, Contrastive loss_step=1.180]Epoch 0:  21%|██        | 13/63 [00:25<01:37,  1.95s/it, loss=3.64, v_num=104676, Contrastive loss_step=2.510]Epoch 0:  22%|██▏       | 14/63 [00:25<01:28,  1.81s/it, loss=3.64, v_num=104676, Contrastive loss_step=2.510]Epoch 0:  22%|██▏       | 14/63 [00:25<01:28,  1.81s/it, loss=3.48, v_num=104676, Contrastive loss_step=1.450]Epoch 0:  24%|██▍       | 15/63 [00:25<01:21,  1.69s/it, loss=3.48, v_num=104676, Contrastive loss_step=1.450]Epoch 0:  24%|██▍       | 15/63 [00:25<01:21,  1.69s/it, loss=3.44, v_num=104676, Contrastive loss_step=2.840]Epoch 0:  25%|██▌       | 16/63 [00:25<01:14,  1.59s/it, loss=3.44, v_num=104676, Contrastive loss_step=2.840]Epoch 0:  25%|██▌       | 16/63 [00:25<01:14,  1.59s/it, loss=3.43, v_num=104676, Contrastive loss_step=3.270]Epoch 0:  27%|██▋       | 17/63 [00:25<01:08,  1.50s/it, loss=3.43, v_num=104676, Contrastive loss_step=3.270]Epoch 0:  27%|██▋       | 17/63 [00:25<01:08,  1.50s/it, loss=3.23, v_num=104676, Contrastive loss_step=0.0803]Epoch 0:  29%|██▊       | 18/63 [00:25<01:03,  1.42s/it, loss=3.23, v_num=104676, Contrastive loss_step=0.0803]Epoch 0:  29%|██▊       | 18/63 [00:25<01:03,  1.42s/it, loss=3.32, v_num=104676, Contrastive loss_step=4.740] Epoch 0:  30%|███       | 19/63 [00:25<00:59,  1.35s/it, loss=3.32, v_num=104676, Contrastive loss_step=4.740]Epoch 0:  30%|███       | 19/63 [00:25<00:59,  1.35s/it, loss=3.27, v_num=104676, Contrastive loss_step=2.360]Epoch 0:  32%|███▏      | 20/63 [00:25<00:55,  1.28s/it, loss=3.27, v_num=104676, Contrastive loss_step=2.360]Epoch 0:  32%|███▏      | 20/63 [00:25<00:55,  1.28s/it, loss=3.2, v_num=104676, Contrastive loss_step=1.900] Epoch 0:  33%|███▎      | 21/63 [00:25<00:51,  1.22s/it, loss=3.2, v_num=104676, Contrastive loss_step=1.900]Epoch 0:  33%|███▎      | 21/63 [00:25<00:51,  1.22s/it, loss=3.15, v_num=104676, Contrastive loss_step=3.030]Epoch 0:  35%|███▍      | 22/63 [00:25<00:47,  1.17s/it, loss=3.15, v_num=104676, Contrastive loss_step=3.030]Epoch 0:  35%|███▍      | 22/63 [00:25<00:47,  1.17s/it, loss=2.91, v_num=104676, Contrastive loss_step=0.533]Epoch 0:  37%|███▋      | 23/63 [00:25<00:44,  1.12s/it, loss=2.91, v_num=104676, Contrastive loss_step=0.533]Epoch 0:  37%|███▋      | 23/63 [00:25<00:44,  1.12s/it, loss=2.79, v_num=104676, Contrastive loss_step=1.980]Epoch 0:  38%|███▊      | 24/63 [00:25<00:41,  1.07s/it, loss=2.79, v_num=104676, Contrastive loss_step=1.980]Epoch 0:  38%|███▊      | 24/63 [00:25<00:41,  1.07s/it, loss=2.79, v_num=104676, Contrastive loss_step=3.040]Epoch 0:  40%|███▉      | 25/63 [00:25<00:39,  1.03s/it, loss=2.79, v_num=104676, Contrastive loss_step=3.040]Epoch 0:  40%|███▉      | 25/63 [00:25<00:39,  1.03s/it, loss=2.85, v_num=104676, Contrastive loss_step=3.250]Epoch 0:  41%|████▏     | 26/63 [00:25<00:36,  1.01it/s, loss=2.85, v_num=104676, Contrastive loss_step=3.250]Epoch 0:  41%|████▏     | 26/63 [00:25<00:36,  1.01it/s, loss=2.64, v_num=104676, Contrastive loss_step=0.789]Epoch 0:  43%|████▎     | 27/63 [00:25<00:34,  1.04it/s, loss=2.64, v_num=104676, Contrastive loss_step=0.789]Epoch 0:  43%|████▎     | 27/63 [00:25<00:34,  1.04it/s, loss=2.69, v_num=104676, Contrastive loss_step=4.250]Epoch 0:  44%|████▍     | 28/63 [00:25<00:32,  1.08it/s, loss=2.69, v_num=104676, Contrastive loss_step=4.250]Epoch 0:  44%|████▍     | 28/63 [00:25<00:32,  1.08it/s, loss=2.58, v_num=104676, Contrastive loss_step=3.770]Epoch 0:  46%|████▌     | 29/63 [00:25<00:30,  1.12it/s, loss=2.58, v_num=104676, Contrastive loss_step=3.770]Epoch 0:  46%|████▌     | 29/63 [00:25<00:30,  1.12it/s, loss=2.64, v_num=104676, Contrastive loss_step=3.250]Epoch 0:  48%|████▊     | 30/63 [00:26<00:28,  1.15it/s, loss=2.64, v_num=104676, Contrastive loss_step=3.250]Epoch 0:  48%|████▊     | 30/63 [00:26<00:28,  1.15it/s, loss=2.64, v_num=104676, Contrastive loss_step=3.600]Epoch 0:  49%|████▉     | 31/63 [00:26<00:26,  1.19it/s, loss=2.64, v_num=104676, Contrastive loss_step=3.600]Epoch 0:  49%|████▉     | 31/63 [00:26<00:26,  1.19it/s, loss=2.64, v_num=104676, Contrastive loss_step=5.080]Epoch 0:  51%|█████     | 32/63 [00:26<00:25,  1.22it/s, loss=2.64, v_num=104676, Contrastive loss_step=5.080]Epoch 0:  51%|█████     | 32/63 [00:26<00:25,  1.22it/s, loss=2.7, v_num=104676, Contrastive loss_step=2.210] Epoch 0:  52%|█████▏    | 33/63 [00:26<00:23,  1.26it/s, loss=2.7, v_num=104676, Contrastive loss_step=2.210]Epoch 0:  52%|█████▏    | 33/63 [00:26<00:23,  1.26it/s, loss=2.9, v_num=104676, Contrastive loss_step=6.650]Epoch 0:  54%|█████▍    | 34/63 [00:26<00:22,  1.30it/s, loss=2.9, v_num=104676, Contrastive loss_step=6.650]Epoch 0:  54%|█████▍    | 34/63 [00:26<00:22,  1.30it/s, loss=3.12, v_num=104676, Contrastive loss_step=5.760]Epoch 0:  56%|█████▌    | 35/63 [00:26<00:21,  1.33it/s, loss=3.12, v_num=104676, Contrastive loss_step=5.760]Epoch 0:  56%|█████▌    | 35/63 [00:26<00:21,  1.33it/s, loss=2.98, v_num=104676, Contrastive loss_step=0.0486]Epoch 0:  57%|█████▋    | 36/63 [00:26<00:19,  1.37it/s, loss=2.98, v_num=104676, Contrastive loss_step=0.0486]Epoch 0:  57%|█████▋    | 36/63 [00:26<00:19,  1.37it/s, loss=2.86, v_num=104676, Contrastive loss_step=0.845] Epoch 0:  59%|█████▊    | 37/63 [00:26<00:18,  1.40it/s, loss=2.86, v_num=104676, Contrastive loss_step=0.845]Epoch 0:  59%|█████▊    | 37/63 [00:26<00:18,  1.40it/s, loss=3.07, v_num=104676, Contrastive loss_step=4.310]Epoch 0:  60%|██████    | 38/63 [00:26<00:17,  1.44it/s, loss=3.07, v_num=104676, Contrastive loss_step=4.310]Epoch 0:  60%|██████    | 38/63 [00:26<00:17,  1.44it/s, loss=2.91, v_num=104676, Contrastive loss_step=1.560]Epoch 0:  62%|██████▏   | 39/63 [00:26<00:16,  1.48it/s, loss=2.91, v_num=104676, Contrastive loss_step=1.560]Epoch 0:  62%|██████▏   | 39/63 [00:26<00:16,  1.48it/s, loss=3.12, v_num=104676, Contrastive loss_step=6.460]Epoch 0:  63%|██████▎   | 40/63 [00:26<00:15,  1.51it/s, loss=3.12, v_num=104676, Contrastive loss_step=6.460]Epoch 0:  63%|██████▎   | 40/63 [00:26<00:15,  1.51it/s, loss=3.06, v_num=104676, Contrastive loss_step=0.806]Epoch 0:  65%|██████▌   | 41/63 [00:26<00:14,  1.55it/s, loss=3.06, v_num=104676, Contrastive loss_step=0.806]Epoch 0:  65%|██████▌   | 41/63 [00:26<00:14,  1.55it/s, loss=3.16, v_num=104676, Contrastive loss_step=5.100]Epoch 0:  67%|██████▋   | 42/63 [00:26<00:13,  1.58it/s, loss=3.16, v_num=104676, Contrastive loss_step=5.100]Epoch 0:  67%|██████▋   | 42/63 [00:26<00:13,  1.58it/s, loss=3.27, v_num=104676, Contrastive loss_step=2.680]Epoch 0:  68%|██████▊   | 43/63 [00:26<00:12,  1.62it/s, loss=3.27, v_num=104676, Contrastive loss_step=2.680]Epoch 0:  68%|██████▊   | 43/63 [00:26<00:12,  1.62it/s, loss=3.23, v_num=104676, Contrastive loss_step=1.050]Epoch 0:  70%|██████▉   | 44/63 [00:26<00:11,  1.65it/s, loss=3.23, v_num=104676, Contrastive loss_step=1.050]Epoch 0:  70%|██████▉   | 44/63 [00:26<00:11,  1.65it/s, loss=3.12, v_num=104676, Contrastive loss_step=0.853]Epoch 0:  71%|███████▏  | 45/63 [00:26<00:10,  1.69it/s, loss=3.12, v_num=104676, Contrastive loss_step=0.853]Epoch 0:  71%|███████▏  | 45/63 [00:26<00:10,  1.69it/s, loss=3.04, v_num=104676, Contrastive loss_step=1.800]Epoch 0:  73%|███████▎  | 46/63 [00:26<00:09,  1.72it/s, loss=3.04, v_num=104676, Contrastive loss_step=1.800]Epoch 0:  73%|███████▎  | 46/63 [00:26<00:09,  1.72it/s, loss=3.02, v_num=104676, Contrastive loss_step=0.311]Epoch 0:  75%|███████▍  | 47/63 [00:26<00:09,  1.76it/s, loss=3.02, v_num=104676, Contrastive loss_step=0.311]Epoch 0:  75%|███████▍  | 47/63 [00:26<00:09,  1.76it/s, loss=2.86, v_num=104676, Contrastive loss_step=1.010]Epoch 0:  76%|███████▌  | 48/63 [00:26<00:08,  1.79it/s, loss=2.86, v_num=104676, Contrastive loss_step=1.010]Epoch 0:  76%|███████▌  | 48/63 [00:26<00:08,  1.79it/s, loss=2.89, v_num=104676, Contrastive loss_step=4.310]Epoch 0:  78%|███████▊  | 49/63 [00:26<00:07,  1.83it/s, loss=2.89, v_num=104676, Contrastive loss_step=4.310]Epoch 0:  78%|███████▊  | 49/63 [00:26<00:07,  1.83it/s, loss=2.94, v_num=104676, Contrastive loss_step=4.400]Epoch 0:  79%|███████▉  | 50/63 [00:26<00:06,  1.86it/s, loss=2.94, v_num=104676, Contrastive loss_step=4.400]Epoch 0:  79%|███████▉  | 50/63 [00:26<00:06,  1.86it/s, loss=2.95, v_num=104676, Contrastive loss_step=3.840]Epoch 0:  81%|████████  | 51/63 [00:26<00:06,  1.90it/s, loss=2.95, v_num=104676, Contrastive loss_step=3.840]Epoch 0:  81%|████████  | 51/63 [00:26<00:06,  1.90it/s, loss=2.93, v_num=104676, Contrastive loss_step=4.600]Epoch 0:  83%|████████▎ | 52/63 [00:26<00:05,  1.93it/s, loss=2.93, v_num=104676, Contrastive loss_step=4.600]Epoch 0:  83%|████████▎ | 52/63 [00:26<00:05,  1.93it/s, loss=2.95, v_num=104676, Contrastive loss_step=2.680]Epoch 0:  84%|████████▍ | 53/63 [00:26<00:05,  1.97it/s, loss=2.95, v_num=104676, Contrastive loss_step=2.680]Epoch 0:  84%|████████▍ | 53/63 [00:26<00:05,  1.97it/s, loss=2.68, v_num=104676, Contrastive loss_step=1.150]Epoch 0:  86%|████████▌ | 54/63 [00:26<00:04,  2.00it/s, loss=2.68, v_num=104676, Contrastive loss_step=1.150]Epoch 0:  86%|████████▌ | 54/63 [00:26<00:04,  2.00it/s, loss=2.67, v_num=104676, Contrastive loss_step=5.500]Epoch 0:  87%|████████▋ | 55/63 [00:26<00:03,  2.04it/s, loss=2.67, v_num=104676, Contrastive loss_step=5.500]Epoch 0:  87%|████████▋ | 55/63 [00:26<00:03,  2.04it/s, loss=2.77, v_num=104676, Contrastive loss_step=2.130]Epoch 0:  89%|████████▉ | 56/63 [00:27<00:03,  2.07it/s, loss=2.77, v_num=104676, Contrastive loss_step=2.130]Epoch 0:  89%|████████▉ | 56/63 [00:27<00:03,  2.07it/s, loss=2.99, v_num=104676, Contrastive loss_step=5.200]Epoch 0:  90%|█████████ | 57/63 [00:27<00:02,  2.11it/s, loss=2.99, v_num=104676, Contrastive loss_step=5.200]Epoch 0:  90%|█████████ | 57/63 [00:27<00:02,  2.11it/s, loss=2.98, v_num=104676, Contrastive loss_step=4.080]Epoch 0:  92%|█████████▏| 58/63 [00:27<00:02,  2.14it/s, loss=2.98, v_num=104676, Contrastive loss_step=4.080]Epoch 0:  92%|█████████▏| 58/63 [00:27<00:02,  2.14it/s, loss=2.97, v_num=104676, Contrastive loss_step=1.380]Epoch 0:  94%|█████████▎| 59/63 [00:27<00:01,  2.18it/s, loss=2.97, v_num=104676, Contrastive loss_step=1.380]Epoch 0:  94%|█████████▎| 59/63 [00:27<00:01,  2.18it/s, loss=2.82, v_num=104676, Contrastive loss_step=3.440]Epoch 0:  95%|█████████▌| 60/63 [00:27<00:01,  2.21it/s, loss=2.82, v_num=104676, Contrastive loss_step=3.440]Epoch 0:  95%|█████████▌| 60/63 [00:27<00:01,  2.21it/s, loss=2.97, v_num=104676, Contrastive loss_step=3.830]Epoch 0:  97%|█████████▋| 61/63 [00:27<00:00,  2.24it/s, loss=2.97, v_num=104676, Contrastive loss_step=3.830]Epoch 0:  97%|█████████▋| 61/63 [00:27<00:00,  2.24it/s, loss=2.95, v_num=104676, Contrastive loss_step=4.730]Epoch 0:  98%|█████████▊| 62/63 [00:27<00:00,  2.28it/s, loss=2.95, v_num=104676, Contrastive loss_step=4.730]Epoch 0:  98%|█████████▊| 62/63 [00:27<00:00,  2.28it/s, loss=3.22, v_num=104676, Contrastive loss_step=8.060]Epoch steps must be integer but is 175.0; i.e. len(data) / effective batch size needs to be a whole number
Attempting to change grad_accum_steps of backbone to 1 to resolve error...
Epoch steps now is 175.0
Adjusted dimensions for backbone OK. Note that code for training backbone computes effective batch size per GPU.
{'path_lbl': 'labeled_data/', 'path_unlbl': 'unlabeled_data/', 'shuffle': False, 'verbose_off': True, 'classes': 100, 'train_backbone': True, 'output_size': 1, 'backbone': 'SimCLR', 'batch_size': 64, 'epochs': 5, 'learn_rate': 0.001, 'momentum': 0.9, 'weight_decay': 0.0005, 'print_freq': 200, 'step': 50, 'gamma': 0.2, 'freeze': False, 'backbone_epochs': 1, 'backbone_cuda_off': True, 'backbone_seed': 77777, 'backbone_img_size': 224, 'backbone_save_directory': 'saved_models/', 'backbone_load_pretrained': False, 'backbone_grad_accumulate_steps': 1, 'backbone_batch_size': 4, 'backbone_embedding_size': 128, 'backbone_lr': 0.003, 'backbone_weight_decay': 1e-06, 'backbone_temperature': 0.1, 'backbone_checkpoint_path': './SimCLR_ResNet18.ckpt', 'backbone_resume': False}
Total effective batch size (across all GPU(s)) is 16
Pretraining backbone...
available_gpus: 4
Dim MLP input: 512
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name  | Type            | Params
------------------------------------------
0 | model | AddProjection   | 11.5 M
1 | loss  | ContrastiveLoss | 0     
------------------------------------------
11.5 M    Trainable params
0         Non-trainable params
11.5 M    Total params
46.024    Total estimated model params size (MB)
Optimizer Adam, Learning Rate 0.003, Effective batch size 4
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/63 [00:00<?, ?it/s] Optimizer Adam, Learning Rate 0.003, Effective batch size 4
Optimizer Adam, Learning Rate 0.003, Effective batch size 4
Optimizer Adam, Learning Rate 0.003, Effective batch size 4
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Epoch 0:   2%|▏         | 1/63 [00:12<13:00, 12.59s/it]Epoch 0:   2%|▏         | 1/63 [00:12<13:06, 12.68s/it, loss=4.08, v_num=104676, Contrastive loss_step=4.080]Epoch 0:   3%|▎         | 2/63 [00:12<06:28,  6.36s/it, loss=4.08, v_num=104676, Contrastive loss_step=4.080]Epoch 0:   3%|▎         | 2/63 [00:12<06:28,  6.37s/it, loss=4.67, v_num=104676, Contrastive loss_step=5.250]Epoch 0:   5%|▍         | 3/63 [00:12<04:15,  4.26s/it, loss=4.67, v_num=104676, Contrastive loss_step=5.250]Epoch 0:   5%|▍         | 3/63 [00:12<04:15,  4.26s/it, loss=4.55, v_num=104676, Contrastive loss_step=4.320]Epoch 0:   6%|▋         | 4/63 [00:12<03:09,  3.21s/it, loss=4.55, v_num=104676, Contrastive loss_step=4.320]Epoch 0:   6%|▋         | 4/63 [00:12<03:09,  3.21s/it, loss=4.18, v_num=104676, Contrastive loss_step=3.040]Epoch 0:   8%|▊         | 5/63 [00:12<02:29,  2.58s/it, loss=4.18, v_num=104676, Contrastive loss_step=3.040]Epoch 0:   8%|▊         | 5/63 [00:12<02:29,  2.58s/it, loss=3.75, v_num=104676, Contrastive loss_step=2.070]Epoch 0:  10%|▉         | 6/63 [00:12<02:02,  2.15s/it, loss=3.75, v_num=104676, Contrastive loss_step=2.070]Epoch 0:  10%|▉         | 6/63 [00:12<02:02,  2.15s/it, loss=3.95, v_num=104676, Contrastive loss_step=4.950]Epoch 0:  11%|█         | 7/63 [00:13<01:45,  1.89s/it, loss=3.95, v_num=104676, Contrastive loss_step=4.950]Epoch 0:  11%|█         | 7/63 [00:13<01:45,  1.89s/it, loss=3.87, v_num=104676, Contrastive loss_step=3.360]Epoch 0:  13%|█▎        | 8/63 [00:13<01:31,  1.66s/it, loss=3.87, v_num=104676, Contrastive loss_step=3.360]Epoch 0:  13%|█▎        | 8/63 [00:13<01:31,  1.66s/it, loss=4.13, v_num=104676, Contrastive loss_step=5.940]Epoch 0:  14%|█▍        | 9/63 [00:13<01:20,  1.48s/it, loss=4.13, v_num=104676, Contrastive loss_step=5.940]Epoch 0:  14%|█▍        | 9/63 [00:13<01:20,  1.48s/it, loss=3.9, v_num=104676, Contrastive loss_step=2.100] Epoch 0:  16%|█▌        | 10/63 [00:13<01:10,  1.34s/it, loss=3.9, v_num=104676, Contrastive loss_step=2.100]Epoch 0:  16%|█▌        | 10/63 [00:13<01:10,  1.34s/it, loss=3.87, v_num=104676, Contrastive loss_step=3.550]Epoch 0:  17%|█▋        | 11/63 [00:13<01:03,  1.22s/it, loss=3.87, v_num=104676, Contrastive loss_step=3.550]Epoch 0:  17%|█▋        | 11/63 [00:13<01:03,  1.22s/it, loss=3.97, v_num=104676, Contrastive loss_step=4.980]Epoch 0:  19%|█▉        | 12/63 [00:13<00:57,  1.12s/it, loss=3.97, v_num=104676, Contrastive loss_step=4.980]Epoch 0:  19%|█▉        | 12/63 [00:13<00:57,  1.12s/it, loss=3.74, v_num=104676, Contrastive loss_step=1.180]Epoch 0:  21%|██        | 13/63 [00:13<00:52,  1.04s/it, loss=3.74, v_num=104676, Contrastive loss_step=1.180]Epoch 0:  21%|██        | 13/63 [00:13<00:52,  1.04s/it, loss=3.64, v_num=104676, Contrastive loss_step=2.510]Epoch 0:  22%|██▏       | 14/63 [00:13<00:48,  1.00it/s, loss=3.64, v_num=104676, Contrastive loss_step=2.510]Epoch 0:  22%|██▏       | 14/63 [00:13<00:48,  1.00it/s, loss=3.48, v_num=104676, Contrastive loss_step=1.450]Epoch 0:  24%|██▍       | 15/63 [00:14<00:44,  1.07it/s, loss=3.48, v_num=104676, Contrastive loss_step=1.450]Epoch 0:  24%|██▍       | 15/63 [00:14<00:44,  1.07it/s, loss=3.44, v_num=104676, Contrastive loss_step=2.840]Epoch 0:  25%|██▌       | 16/63 [00:14<00:41,  1.14it/s, loss=3.44, v_num=104676, Contrastive loss_step=2.840]Epoch 0:  25%|██▌       | 16/63 [00:14<00:41,  1.14it/s, loss=3.43, v_num=104676, Contrastive loss_step=3.270]Epoch 0:  27%|██▋       | 17/63 [00:14<00:38,  1.20it/s, loss=3.43, v_num=104676, Contrastive loss_step=3.270]Epoch 0:  27%|██▋       | 17/63 [00:14<00:38,  1.20it/s, loss=3.23, v_num=104676, Contrastive loss_step=0.0803]Epoch 0:  29%|██▊       | 18/63 [00:14<00:35,  1.27it/s, loss=3.23, v_num=104676, Contrastive loss_step=0.0803]Epoch 0:  29%|██▊       | 18/63 [00:14<00:35,  1.27it/s, loss=3.32, v_num=104676, Contrastive loss_step=4.740] Epoch 0:  30%|███       | 19/63 [00:14<00:32,  1.34it/s, loss=3.32, v_num=104676, Contrastive loss_step=4.740]Epoch 0:  30%|███       | 19/63 [00:14<00:32,  1.34it/s, loss=3.27, v_num=104676, Contrastive loss_step=2.360]Epoch 0:  32%|███▏      | 20/63 [00:14<00:30,  1.40it/s, loss=3.27, v_num=104676, Contrastive loss_step=2.360]Epoch 0:  32%|███▏      | 20/63 [00:14<00:30,  1.40it/s, loss=3.2, v_num=104676, Contrastive loss_step=1.900] Epoch 0:  33%|███▎      | 21/63 [00:14<00:28,  1.47it/s, loss=3.2, v_num=104676, Contrastive loss_step=1.900]Epoch 0:  33%|███▎      | 21/63 [00:14<00:28,  1.47it/s, loss=3.15, v_num=104676, Contrastive loss_step=3.030]Epoch 0:  35%|███▍      | 22/63 [00:14<00:26,  1.53it/s, loss=3.15, v_num=104676, Contrastive loss_step=3.030]Epoch 0:  35%|███▍      | 22/63 [00:14<00:26,  1.53it/s, loss=2.91, v_num=104676, Contrastive loss_step=0.533]Epoch 0:  37%|███▋      | 23/63 [00:14<00:25,  1.60it/s, loss=2.91, v_num=104676, Contrastive loss_step=0.533]Epoch 0:  37%|███▋      | 23/63 [00:14<00:25,  1.60it/s, loss=2.79, v_num=104676, Contrastive loss_step=1.980]Epoch 0:  38%|███▊      | 24/63 [00:14<00:23,  1.66it/s, loss=2.79, v_num=104676, Contrastive loss_step=1.980]Epoch 0:  38%|███▊      | 24/63 [00:14<00:23,  1.66it/s, loss=2.79, v_num=104676, Contrastive loss_step=3.040]Epoch 0:  40%|███▉      | 25/63 [00:14<00:22,  1.73it/s, loss=2.79, v_num=104676, Contrastive loss_step=3.040]Epoch 0:  40%|███▉      | 25/63 [00:14<00:22,  1.73it/s, loss=2.85, v_num=104676, Contrastive loss_step=3.250]Epoch 0:  41%|████▏     | 26/63 [00:14<00:20,  1.79it/s, loss=2.85, v_num=104676, Contrastive loss_step=3.250]Epoch 0:  41%|████▏     | 26/63 [00:14<00:20,  1.79it/s, loss=2.64, v_num=104676, Contrastive loss_step=0.789]Epoch 0:  43%|████▎     | 27/63 [00:14<00:19,  1.85it/s, loss=2.64, v_num=104676, Contrastive loss_step=0.789]Epoch 0:  43%|████▎     | 27/63 [00:14<00:19,  1.85it/s, loss=2.69, v_num=104676, Contrastive loss_step=4.250]Epoch 0:  44%|████▍     | 28/63 [00:14<00:18,  1.91it/s, loss=2.69, v_num=104676, Contrastive loss_step=4.250]Epoch 0:  44%|████▍     | 28/63 [00:14<00:18,  1.91it/s, loss=2.58, v_num=104676, Contrastive loss_step=3.770]Epoch 0:  46%|████▌     | 29/63 [00:14<00:17,  1.98it/s, loss=2.58, v_num=104676, Contrastive loss_step=3.770]Epoch 0:  46%|████▌     | 29/63 [00:14<00:17,  1.98it/s, loss=2.64, v_num=104676, Contrastive loss_step=3.250]Epoch 0:  48%|████▊     | 30/63 [00:14<00:16,  2.04it/s, loss=2.64, v_num=104676, Contrastive loss_step=3.250]Epoch 0:  48%|████▊     | 30/63 [00:14<00:16,  2.04it/s, loss=2.64, v_num=104676, Contrastive loss_step=3.600]Epoch 0:  49%|████▉     | 31/63 [00:14<00:15,  2.10it/s, loss=2.64, v_num=104676, Contrastive loss_step=3.600]Epoch 0:  49%|████▉     | 31/63 [00:14<00:15,  2.10it/s, loss=2.64, v_num=104676, Contrastive loss_step=5.080]Epoch 0:  51%|█████     | 32/63 [00:14<00:14,  2.16it/s, loss=2.64, v_num=104676, Contrastive loss_step=5.080]Epoch 0:  51%|█████     | 32/63 [00:14<00:14,  2.16it/s, loss=2.7, v_num=104676, Contrastive loss_step=2.210] Epoch 0:  52%|█████▏    | 33/63 [00:14<00:13,  2.22it/s, loss=2.7, v_num=104676, Contrastive loss_step=2.210]Epoch 0:  52%|█████▏    | 33/63 [00:14<00:13,  2.22it/s, loss=2.9, v_num=104676, Contrastive loss_step=6.650]Epoch 0:  54%|█████▍    | 34/63 [00:14<00:12,  2.28it/s, loss=2.9, v_num=104676, Contrastive loss_step=6.650]Epoch 0:  54%|█████▍    | 34/63 [00:14<00:12,  2.28it/s, loss=3.12, v_num=104676, Contrastive loss_step=5.760]Epoch 0:  56%|█████▌    | 35/63 [00:14<00:11,  2.34it/s, loss=3.12, v_num=104676, Contrastive loss_step=5.760]Epoch 0:  56%|█████▌    | 35/63 [00:14<00:11,  2.34it/s, loss=2.98, v_num=104676, Contrastive loss_step=0.0486]Epoch 0:  57%|█████▋    | 36/63 [00:14<00:11,  2.40it/s, loss=2.98, v_num=104676, Contrastive loss_step=0.0486]Epoch 0:  57%|█████▋    | 36/63 [00:14<00:11,  2.40it/s, loss=2.86, v_num=104676, Contrastive loss_step=0.845] Epoch 0:  59%|█████▊    | 37/63 [00:15<00:10,  2.46it/s, loss=2.86, v_num=104676, Contrastive loss_step=0.845]Epoch 0:  59%|█████▊    | 37/63 [00:15<00:10,  2.46it/s, loss=3.07, v_num=104676, Contrastive loss_step=4.310]Epoch 0:  60%|██████    | 38/63 [00:15<00:09,  2.52it/s, loss=3.07, v_num=104676, Contrastive loss_step=4.310]Epoch 0:  60%|██████    | 38/63 [00:15<00:09,  2.52it/s, loss=2.91, v_num=104676, Contrastive loss_step=1.560]Epoch 0:  62%|██████▏   | 39/63 [00:15<00:09,  2.58it/s, loss=2.91, v_num=104676, Contrastive loss_step=1.560]Epoch 0:  62%|██████▏   | 39/63 [00:15<00:09,  2.58it/s, loss=3.12, v_num=104676, Contrastive loss_step=6.460]Epoch 0:  63%|██████▎   | 40/63 [00:15<00:08,  2.64it/s, loss=3.12, v_num=104676, Contrastive loss_step=6.460]Epoch 0:  63%|██████▎   | 40/63 [00:15<00:08,  2.64it/s, loss=3.06, v_num=104676, Contrastive loss_step=0.806]Epoch 0:  65%|██████▌   | 41/63 [00:15<00:08,  2.70it/s, loss=3.06, v_num=104676, Contrastive loss_step=0.806]Epoch 0:  65%|██████▌   | 41/63 [00:15<00:08,  2.70it/s, loss=3.16, v_num=104676, Contrastive loss_step=5.100]Epoch 0:  67%|██████▋   | 42/63 [00:15<00:07,  2.76it/s, loss=3.16, v_num=104676, Contrastive loss_step=5.100]Epoch 0:  67%|██████▋   | 42/63 [00:15<00:07,  2.76it/s, loss=3.27, v_num=104676, Contrastive loss_step=2.680]Epoch 0:  68%|██████▊   | 43/63 [00:15<00:07,  2.82it/s, loss=3.27, v_num=104676, Contrastive loss_step=2.680]Epoch 0:  68%|██████▊   | 43/63 [00:15<00:07,  2.82it/s, loss=3.23, v_num=104676, Contrastive loss_step=1.050]Epoch 0:  70%|██████▉   | 44/63 [00:15<00:06,  2.87it/s, loss=3.23, v_num=104676, Contrastive loss_step=1.050]Epoch 0:  70%|██████▉   | 44/63 [00:15<00:06,  2.87it/s, loss=3.12, v_num=104676, Contrastive loss_step=0.853]Epoch 0:  71%|███████▏  | 45/63 [00:15<00:06,  2.93it/s, loss=3.12, v_num=104676, Contrastive loss_step=0.853]Epoch 0:  71%|███████▏  | 45/63 [00:15<00:06,  2.93it/s, loss=3.04, v_num=104676, Contrastive loss_step=1.800]Epoch 0:  73%|███████▎  | 46/63 [00:15<00:05,  2.99it/s, loss=3.04, v_num=104676, Contrastive loss_step=1.800]Epoch 0:  73%|███████▎  | 46/63 [00:15<00:05,  2.99it/s, loss=3.02, v_num=104676, Contrastive loss_step=0.311]Epoch 0:  75%|███████▍  | 47/63 [00:15<00:05,  3.05it/s, loss=3.02, v_num=104676, Contrastive loss_step=0.311]Epoch 0:  75%|███████▍  | 47/63 [00:15<00:05,  3.05it/s, loss=2.86, v_num=104676, Contrastive loss_step=1.010]Epoch 0:  76%|███████▌  | 48/63 [00:15<00:04,  3.11it/s, loss=2.86, v_num=104676, Contrastive loss_step=1.010]Epoch 0:  76%|███████▌  | 48/63 [00:15<00:04,  3.11it/s, loss=2.89, v_num=104676, Contrastive loss_step=4.310]Epoch 0:  78%|███████▊  | 49/63 [00:15<00:04,  3.17it/s, loss=2.89, v_num=104676, Contrastive loss_step=4.310]Epoch 0:  78%|███████▊  | 49/63 [00:15<00:04,  3.17it/s, loss=2.94, v_num=104676, Contrastive loss_step=4.400]Epoch 0:  79%|███████▉  | 50/63 [00:15<00:04,  3.23it/s, loss=2.94, v_num=104676, Contrastive loss_step=4.400]Epoch 0:  79%|███████▉  | 50/63 [00:15<00:04,  3.23it/s, loss=2.95, v_num=104676, Contrastive loss_step=3.840]Epoch 0:  81%|████████  | 51/63 [00:15<00:03,  3.28it/s, loss=2.95, v_num=104676, Contrastive loss_step=3.840]Epoch 0:  81%|████████  | 51/63 [00:15<00:03,  3.28it/s, loss=2.93, v_num=104676, Contrastive loss_step=4.600]Epoch 0:  83%|████████▎ | 52/63 [00:15<00:03,  3.34it/s, loss=2.93, v_num=104676, Contrastive loss_step=4.600]Epoch 0:  83%|████████▎ | 52/63 [00:15<00:03,  3.34it/s, loss=2.95, v_num=104676, Contrastive loss_step=2.680]Epoch 0:  84%|████████▍ | 53/63 [00:15<00:02,  3.40it/s, loss=2.95, v_num=104676, Contrastive loss_step=2.680]Epoch 0:  84%|████████▍ | 53/63 [00:15<00:02,  3.40it/s, loss=2.68, v_num=104676, Contrastive loss_step=1.150]Epoch 0:  86%|████████▌ | 54/63 [00:15<00:02,  3.45it/s, loss=2.68, v_num=104676, Contrastive loss_step=1.150]Epoch 0:  86%|████████▌ | 54/63 [00:15<00:02,  3.45it/s, loss=2.67, v_num=104676, Contrastive loss_step=5.500]Epoch 0:  87%|████████▋ | 55/63 [00:15<00:02,  3.51it/s, loss=2.67, v_num=104676, Contrastive loss_step=5.500]Epoch 0:  87%|████████▋ | 55/63 [00:15<00:02,  3.51it/s, loss=2.77, v_num=104676, Contrastive loss_step=2.130]Epoch 0:  89%|████████▉ | 56/63 [00:15<00:01,  3.57it/s, loss=2.77, v_num=104676, Contrastive loss_step=2.130]Epoch 0:  89%|████████▉ | 56/63 [00:15<00:01,  3.57it/s, loss=2.99, v_num=104676, Contrastive loss_step=5.200]Epoch 0:  90%|█████████ | 57/63 [00:15<00:01,  3.62it/s, loss=2.99, v_num=104676, Contrastive loss_step=5.200]Epoch 0:  90%|█████████ | 57/63 [00:15<00:01,  3.62it/s, loss=2.98, v_num=104676, Contrastive loss_step=4.080]Epoch 0:  92%|█████████▏| 58/63 [00:15<00:01,  3.68it/s, loss=2.98, v_num=104676, Contrastive loss_step=4.080]Epoch 0:  92%|█████████▏| 58/63 [00:15<00:01,  3.68it/s, loss=2.97, v_num=104676, Contrastive loss_step=1.380]Epoch 0:  94%|█████████▎| 59/63 [00:15<00:01,  3.73it/s, loss=2.97, v_num=104676, Contrastive loss_step=1.380]Epoch 0:  94%|█████████▎| 59/63 [00:15<00:01,  3.73it/s, loss=2.82, v_num=104676, Contrastive loss_step=3.440]Epoch 0:  95%|█████████▌| 60/63 [00:15<00:00,  3.79it/s, loss=2.82, v_num=104676, Contrastive loss_step=3.440]Epoch 0:  95%|█████████▌| 60/63 [00:15<00:00,  3.79it/s, loss=2.97, v_num=104676, Contrastive loss_step=3.830]Epoch 0:  97%|█████████▋| 61/63 [00:15<00:00,  3.85it/s, loss=2.97, v_num=104676, Contrastive loss_step=3.830]Epoch 0:  97%|█████████▋| 61/63 [00:15<00:00,  3.84it/s, loss=2.95, v_num=104676, Contrastive loss_step=4.730]Epoch 0:  98%|█████████▊| 62/63 [00:15<00:00,  3.90it/s, loss=2.95, v_num=104676, Contrastive loss_step=4.730]Epoch 0:  98%|█████████▊| 62/63 [00:15<00:00,  3.90it/s, loss=3.22, v_num=104676, Contrastive loss_step=8.060]Could not adjust parameters.
